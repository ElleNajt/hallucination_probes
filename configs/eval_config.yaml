# Example evaluation configuration for hallucination detection probe

# Probe configuration
probe_config:
  probe_id: "llama3_1_8b_lora_lambda_kl=0.5"
  model_name: "meta-llama/Meta-Llama-3.1-8B-Instruct"
  layer: 30
  threshold: 0.5
  load_from: "hf"  # Options: "disk", "hf", or null
  hf_repo_id: "andyrdt/hallucination-probes"

# Batch size for evaluation
per_device_eval_batch_size: 8

# Output configuration
save_predictions: true
save_roc_curves: true
save_raw_results: false

# Datasets to evaluate on
datasets:
  - dataset_id: "llama3_1_8b_longform_test"
    hf_repo: "obalcells/hallucination-heads"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 1536
    pos_weight: 10.0
    neg_weight: 10.0
    default_ignore: false
    shuffle: false

  - dataset_id: "llama3_1_8b_trivia_qa_test"
    hf_repo: "obalcells/triviaqa-balanced"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 128
    pos_weight: 10.0
    neg_weight: 10.0
    default_ignore: false
    shuffle: false

  - dataset_id: "llama3_1_8b_healthbench_test"
    hf_repo: "obalcells/hallucination-heads-healthbench"
    subset: "Meta-Llama-3.1-8B-Instruct"
    split: "test"
    max_length: 1536
    pos_weight: 1.0
    neg_weight: 1.0
    shuffle: false
    shuffle: true
    seed: 0
